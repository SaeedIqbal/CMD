# Causal Meta-Disentanglement (CMD) for Robust Few-Shot Object Detection

[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![PyTorch 1.12+](https://img.shields.io/badge/PyTorch-1.12%2B-red)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-Apache%202.0-green)](LICENSE)

**Official PyTorch implementation** of **"Causal Meta-Disentanglement for Robust Few-Shot Object Detection"**, a novel framework that redefines industrial few-shot object detection as **causal deviation detection**, not class similarity.

> **Key Insight**: Traditional FSOD methods (e.g., DLFG) fail in industrial settings because they conflate **causal defect signals** (e.g., micro-cracks) with **non-causal confounders** (e.g., glare, pose). CMD enforces **causal identifiability** in latent representations, achieving state-of-the-art robustness.

---

## üìå Highlights

- ‚úÖ **First FSOD framework with causal disentanglement**: Explicitly separates \( \mathcal{Z}_c \) (defect generators) from \( \mathcal{Z}_n \) (nuisances).
- ‚úÖ **Meta-pretraining via normality deviation**: Learns defect semantics from **defect-free base data** using procedural corruption.
- ‚úÖ **Interventional similarity \( \mathcal{S}_{\text{do}} \)**: Computes similarity only over causally relevant features, ensuring invariance to confounders.
- ‚úÖ **+4.2% avg nAP50 gain** over SOTA across **7 industrial datasets**.
- ‚úÖ **Robustness gap reduced by 67%** under synthetic confounders (glare, texture, pose).

---

## üìä Results

### Main Results (3-shot, avg over 7 datasets)
| Method       | nAP50 (%) ‚Üë | Pixel AUROC (%) ‚Üë | F1-Score (%) ‚Üë | Robustness Gap ‚Üì |
|--------------|-------------|-------------------|----------------|------------------|
| TFA          | 40.7        | 84.2              | 70.9           | 6.9              |
| Meta R-CNN   | 43.0        | 86.1              | 73.2           | 6.5              |
| DeFRCN       | 45.5        | 87.2              | 74.0           | 6.2              |
| FSCE         | 46.9        | 87.6              | 74.3           | 6.2              |
| **DLFG**     | **49.2**    | **87.9**          | **74.7**       | **6.2**          |
| **CMD (Ours)** | **53.4**    | **92.9**          | **81.5**       | **2.3**          |

> **CMD outperforms DLFG by +4.2% nAP50** and reduces robustness gap from **6.2 ‚Üí 2.3**.

### Per-Dataset nAP50 (3-shot)
| Dataset       | DLFG   | CMD    | Gain   |
|---------------|--------|--------|--------|
| VisA          | 50.4   | 55.1   | +4.7   |
| MVTec-AD      | 52.9   | 57.6   | +4.7   |
| MVTec-LOCO    | 48.3   | 54.4   | **+6.1** |
| Real-IAD      | 48.9   | 54.7   | **+5.8** |
| BTAD          | 49.2   | 53.8   | +4.6   |
| DGAD          | 46.5   | 51.2   | +4.7   |
| MPDD          | 48.1   | 52.9   | +4.8   |

---

## üß† Method Overview

CMD introduces four key innovations:

1. **Causal Latent Space Design**:  
   Structured encoder \( E_\phi: \mathcal{X} \rightarrow \mathcal{Z}_c \oplus \mathcal{Z}_n \) with HSIC + orthogonality losses.

2. **Meta-Pretraining via Normality Deviation**:  
   Simulates defects via procedural corruption (Perlin noise, masking, Gaussian blobs) on normal data.

3. **Few-Shot Adaptation with Causal Transfer**:  
   Meta-attention over base-class priors to infer causal importance vector \( w \).

4. **Interventional Similarity \( \mathcal{S}_{\text{do}} \)**:  
   Cosine similarity only on pruned causal subspace (\( \rho = 0.5 \)).

![CMD Workflow](assets/cmd_workflow.png)

---

## üìÅ Code Structure

```
CMD/
‚îú‚îÄ‚îÄ configs/                  # YAML configs for datasets and training
‚îú‚îÄ‚îÄ data/                     # Dataset loading and corruption
‚îÇ   ‚îú‚îÄ‚îÄ dataset_factory.py
‚îÇ   ‚îú‚îÄ‚îÄ industrial_dataset.py
‚îÇ   ‚îî‚îÄ‚îÄ corruption.py
‚îú‚îÄ‚îÄ models/                   # Core CMD modules
‚îÇ   ‚îú‚îÄ‚îÄ backbone.py
‚îÇ   ‚îú‚îÄ‚îÄ causal_encoder.py
‚îÇ   ‚îú‚îÄ‚îÄ decoder.py
‚îÇ   ‚îú‚îÄ‚îÄ meta_transfer.py
‚îÇ   ‚îî‚îÄ‚îÄ detection_head.py
‚îú‚îÄ‚îÄ losses/                   # Custom losses
‚îÇ   ‚îú‚îÄ‚îÄ vae_loss.py
‚îÇ   ‚îú‚îÄ‚îÄ hsic.py
‚îÇ   ‚îú‚îÄ‚îÄ ortho_loss.py
‚îÇ   ‚îî‚îÄ‚îÄ normality_deviation.py
‚îú‚îÄ‚îÄ utils/                    # Utilities
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ pca_normality.py
‚îÇ   ‚îî‚îÄ‚îÄ confounder_benchmark.py
‚îú‚îÄ‚îÄ scripts/                  # Training and evaluation
‚îÇ   ‚îú‚îÄ‚îÄ train_base.py
‚îÇ   ‚îú‚îÄ‚îÄ train_novel.py
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Installation

```bash
# Clone repository
git clone https://github.com/yourname/CMD.git
cd CMD

# Create conda environment
conda create -n cmd python=3.9
conda activate cmd

# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
pip install perlin-noise  # For procedural corruption
```

---

## üì• Datasets

We evaluate on **7 industrial anomaly detection benchmarks**:

| Dataset       | Classes | Images  | Labels                     | Access |
|---------------|---------|---------|----------------------------|--------|
| **VisA**      | 12      | 10,821  | Bounding boxes             | [GitHub](https://github.com/amazon-science/spot-diff) |
| **MVTec-AD**  | 15      | 5,354   | Pixel masks                | [MVTec](https://www.mvtec.com/company/research/mvtec-ad/) |
| **MVTec-LOCO**| 5       | 3,644   | Pixel masks (logical/structural) | [MVTec](https://www.mvtec.com/company/research/datasets/mvtec-loco/) |
| **Real-IAD**  | 8       | 151,050 | Bounding boxes             | [Email Request](https://realiad4ad.github.io/Real-IAD) |
| **BTAD**      | 3       | 2,540   | Pixel masks                | [Dataset Ninja](https://datasetninja.com/btad) |
| **DGAD**      | 10      | 17,100  | Pixel masks                | [GitHub](https://github.com/mala-lab/GGAD) |
| **MPDD**      | 6       | 1,346   | Bounding boxes             | [GitHub](https://github.com/stepanje/MPDD) |

> **Note**: Place all datasets under `/home/phd/datasets/` as:
> ```
> /home/phd/datasets/
> ‚îú‚îÄ‚îÄ VisA/
> ‚îú‚îÄ‚îÄ MVTec-AD/
> ‚îú‚îÄ‚îÄ MVTec-LOCO/
> ‚îú‚îÄ‚îÄ Real-IAD/
> ‚îú‚îÄ‚îÄ BTAD/
> ‚îú‚îÄ‚îÄ DGAD/
> ‚îî‚îÄ‚îÄ MPDD/
> ```

---

## ‚ñ∂Ô∏è Usage

### 1. Meta-Pretraining on Normal Data
```bash
python scripts/train_base.py --dataset mvtec_ad --epochs 20000
```

### 2. Few-Shot Fine-Tuning
```bash
python scripts/train_novel.py \
    --dataset mvtec_ad \
    --novel_class bottle \
    --k_shot 3 \
    --base_ckpt checkpoints/mvtec_ad_base_model.pth \
    --manifold_path checkpoints/mvtec_ad_manifold.pth
```

### 3. Evaluation
```bash
python scripts/evaluate.py \
    --dataset mvtec_ad \
    --k_shot 3 \
    --checkpoint checkpoints/mvtec_ad_bottle_3shot.pth \
    --base_ckpt checkpoints/mvtec_ad_base_model.pth \
    --manifold_path checkpoints/mvtec_ad_manifold.pth
```

---

## üìö Citation

If you find CMD useful in your research, please cite our paper:

```bibtex
@article{cmd2025,
  title={Causal Meta-Disentanglement for Robust Few-Shot Object Detection},
  author={Saeed Iqbal},
  journal={IEEE Transactions on Industrial Informatics},
  year={2025}
}
```

Also cite the original datasets:

- **VisA**: [Liu et al., CVPR 2022](https://arxiv.org/abs/2206.08988)
- **MVTec-AD/LOCO**: [Bergmann et al., CVPR 2019, 2022](https://www.mvtec.com/company/research/datasets/)
- **Real-IAD**: [Wang et al., CVPR 2024](https://realiad4ad.github.io/Real-IAD)
- **BTAD**: [Mishra et al., ISIE 2021](https://datasetninja.com/btad)
- **DGAD**: [Wang et al., 2022](https://github.com/mala-lab/GGAD)
- **MPDD**: [Je≈æek et al., 2021](https://github.com/stepanje/MPDD)

---

## üìú License

This project is released under the [Apache 2.0 License](LICENSE).

---

## üôè Acknowledgements

- This code builds upon [DLFG](https://ieeexplore.ieee.org/document/10558983) and [Meta R-CNN](https://openaccess.thecvf.com/content_ICCV_2019/html/Yan_Meta_R-CNN_Towards_General_Solver_for_Instance-Level_Low-Shot_Learning_ICCV_2019_paper.html).
- Industrial datasets are provided by their respective authors‚Äîthank you for enabling reproducible research!


--- 

This README is **ready for GitHub** and provides everything a user needs to **reproduce your results**, **extend your method**, or **apply CMD to new industrial datasets**.
